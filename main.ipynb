{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LhOslm_cswec"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Dataset : https://www.kaggle.com/snehaanbhawal/resume-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rX7aN7T2tXSI"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2DFo5vagMO4",
    "outputId": "0dc81236-df8e-406b-fb24-c65ab155d7e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SAYAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SAYAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SAYAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SAYAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ! pip install spacy gensim pyLDAvis wordcloud plotly nltk\n",
    "import nltk\n",
    "nltk.download(['stopwords','wordnet'])\n",
    "#spacy\n",
    "# ! pip install spacy\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "#gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "#Visualization\n",
    "from spacy import displacy\n",
    "import pyLDAvis.gensim_models\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Data loading/ Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# ! pip install jsonlines\n",
    "import jsonlines\n",
    "\n",
    "#nltk\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(['stopwords','wordnet'])\n",
    "\n",
    "#warning\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "## For dropdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import IPython.display\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ex-KFJOGuOYz"
   },
   "source": [
    "# Resume Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zkZ0WlFEtaac"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Resume.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mResume.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1024\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1011\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1012\u001b[39m     dialect,\n\u001b[32m   1013\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1020\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1021\u001b[39m )\n\u001b[32m   1022\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:618\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    615\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    617\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1618\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1615\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1617\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1618\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1878\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1876\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1877\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1878\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1889\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Resume.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Resume.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "v7273AIC5VUf",
    "outputId": "88e64ba3-cb93-4c79-812f-80ada8466e09"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mdf\u001b[49m.reindex(np.random.permutation(df.index))\n\u001b[32m      2\u001b[39m data = df.copy().iloc[\u001b[32m0\u001b[39m:\u001b[32m1000\u001b[39m,]\n\u001b[32m      3\u001b[39m data.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df = df.reindex(np.random.permutation(df.index))\n",
    "data = df.copy().iloc[0:1000,]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0XtXArh518b"
   },
   "outputs": [],
   "source": [
    "! python -m spacy download en_core_web_lg\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHU85zLf8ICz"
   },
   "outputs": [],
   "source": [
    "\n",
    "skill_pattern_path = \"jz_skill_patterns.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entity Ruler**\n",
    "To create an entity ruler we need to add a pipeline and then load the .jsonl file containing skills into ruler. As you can see we have successfully added a new pipeline entity_ruler. Entity ruler helps us add additional rules to highlight various categories within the text, such as skills and job description in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "wJOYyfAl8Iga",
    "outputId": "f968af29-2fa3-41f0-f1ff-1757b240b4ec"
   },
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.from_disk(skill_pattern_path)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Skills**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bOU9-DSH_A88"
   },
   "outputs": [],
   "source": [
    "def get_skills(text):\n",
    "    doc = nlp(text)\n",
    "    myset = []\n",
    "    subset = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"SKILL\":\n",
    "            subset.append(ent.text)\n",
    "    myset.append(subset)\n",
    "    return subset\n",
    "\n",
    "\n",
    "def unique_skills(x):\n",
    "    return list(set(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Resume \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = []\n",
    "for i in range(data.shape[0]):\n",
    "    review = re.sub(\n",
    "        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n",
    "        \" \",\n",
    "        data[\"Resume_str\"].iloc[i],\n",
    "    )\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    lm = WordNetLemmatizer()\n",
    "    review = [\n",
    "        lm.lemmatize(word)\n",
    "        for word in review\n",
    "        if not word in set(stopwords.words(\"english\"))\n",
    "    ]\n",
    "    review = \" \".join(review)\n",
    "    clean.append(review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying functions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Clean_Resume\"] = clean\n",
    "data[\"skills\"] = data[\"Clean_Resume\"].str.lower().apply(get_skills)\n",
    "data[\"skills\"] = data[\"skills\"].apply(unique_skills)\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Analysis And Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data, x=\"Category\", title=\"Distribution of Jobs Categories\"\n",
    ").update_xaxes(categoryorder=\"total descending\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_cat = data[\"Category\"].unique()\n",
    "Job_cat = np.append(Job_cat, \"ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropdown_menu_widget(Job_Category):\n",
    "    output=widgets.Output()\n",
    "    drop_Job=widgets.Dropdown(options = Job_Category, value=None, description='Job Category:')\n",
    "    \n",
    "    for cat in Job_Category:\n",
    "        def dropdown_Job_eventhandler(change):\n",
    "            display(input_widgets)\n",
    "            job_choice=change.new\n",
    "            IPython.display.clear_output(wait=True)\n",
    "    drop_Job.observe(dropdown_Job_eventhandler,names='value')\n",
    "    input_widgets=widgets.HBox([drop_Job])\n",
    "    display(input_widgets)\n",
    "    IPython.display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdown_menu_widget(Job_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_Category=\"INFORMATION-TECHNOLOGY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_skills = []\n",
    "if Job_Category != \"ALL\":\n",
    "    fltr = data[data[\"Category\"] == Job_Category][\"skills\"]\n",
    "    for x in fltr:\n",
    "        for i in x:\n",
    "            Total_skills.append(i)\n",
    "else:\n",
    "    fltr = data[\"skills\"]\n",
    "    for x in fltr:\n",
    "        for i in x:\n",
    "            Total_skills.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    x=Total_skills,\n",
    "    labels={\"x\": \"Skills\"},\n",
    "    title=f\"{Job_Category} Distribution of Skills\",\n",
    ").update_xaxes(categoryorder=\"total descending\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Most common words**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for i in data[data[\"Category\"] == Job_Category][\"Clean_Resume\"].values:\n",
    "    text += i + \" \"\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "x, y = np.ogrid[:300, :300]\n",
    "\n",
    "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = 255 * mask.astype(int)\n",
    "\n",
    "wc = WordCloud(\n",
    "    width=800,\n",
    "    height=800,\n",
    "    background_color=\"white\",\n",
    "    min_font_size=6,\n",
    "    repeat=True,\n",
    "    mask=mask,\n",
    ")\n",
    "wc.generate(text)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.title(f\"Most Used Words in {Job_Category} Resume\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entity Recognition**\n",
    "We can also display various entities within our raw text by using spaCy displacy.render. I am in love with this function as it is an amazing way to look at your entire document and discover SKILL or GEP within your Resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nlp(data[\"Resume_str\"].iloc[0])\n",
    "displacy.render(sent, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dependency Parsing**\n",
    "We can also visualize dependencies by just changing style to dep as shown below. We have also limited words to 10 which includes space too. Limiting the words will make it visualize the small chunk of data and if you want to see the dependency, you can remove the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(sent[0:10], style=\"dep\", jupyter=True, options={\"distance\": 90})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Custom Entity Recognition**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = df.Category.unique()\n",
    "for a in patterns:\n",
    "    ruler.add_patterns([{\"label\": \"Job-Category\", \"pattern\": a}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options=[{\"ents\": \"Job-Category\", \"colors\": \"#ff3232\"},{\"ents\": \"SKILL\", \"colors\": \"#56c426\"}]\n",
    "colors = {\n",
    "    \"Job-Category\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n",
    "    \"SKILL\": \"linear-gradient(90deg, #9BE15D, #00E3AE)\",\n",
    "    \"ORG\": \"#ffd966\",\n",
    "    \"PERSON\": \"#e06666\",\n",
    "    \"GPE\": \"#9fc5e8\",\n",
    "    \"DATE\": \"#c27ba0\",\n",
    "    \"ORDINAL\": \"#674ea7\",\n",
    "    \"PRODUCT\": \"#f9cb9c\",\n",
    "}\n",
    "options = {\n",
    "    \"ents\": [\n",
    "        \"Job-Category\",\n",
    "        \"SKILL\",\n",
    "        \"ORG\",\n",
    "        \"PERSON\",\n",
    "        \"GPE\",\n",
    "        \"DATE\",\n",
    "        \"ORDINAL\",\n",
    "        \"PRODUCT\",\n",
    "    ],\n",
    "    \"colors\": colors,\n",
    "}\n",
    "sent = nlp(data[\"Resume_str\"].iloc[5])\n",
    "displacy.render(sent, style=\"ent\", jupyter=True, options=options)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Resume Analysis**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Single Resume Analyzer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tkinter import ttk\n",
    "\n",
    "# Create a Tkinter root window\n",
    "root = tk.Tk()\n",
    "root.title(\"Resume Recommendation System\")\n",
    "\n",
    "# Define style for the widgets\n",
    "style = ttk.Style()\n",
    "style.configure('TLabel', foreground='black', font=('Arial', 12))\n",
    "style.configure('TButton', foreground='black', font=('Arial', 12))\n",
    "style.configure('TEntry', foreground='black', font=('Arial', 12))\n",
    "\n",
    "# Define a function to handle the file chooser button click event\n",
    "def choose_file():\n",
    "    # Get the desired skills from the entry widget\n",
    "    input_skills = tag_entry.get()\n",
    "\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        # Open the selected PDF file in read binary mode\n",
    "        with open(file_path, 'rb') as pdf_file:\n",
    "            # Create a PDF reader object\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            # Read the text from each page of the PDF file\n",
    "            pdf_text = []\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page_obj = pdf_reader.pages[page_num]\n",
    "                pdf_text.append(page_obj.extract_text())\n",
    "            # Join the text from each page into a single string\n",
    "            input_resume = '\\n'.join(pdf_text)\n",
    "\n",
    "            # Now you can use the 'text' variable to store the extracted text from the PDF file\n",
    "            sent2 = nlp(input_resume)\n",
    "            displacy.render(sent2, style=\"ent\", jupyter=True, options=options)\n",
    "\n",
    "\n",
    "            req_skills = input_skills.lower().split(\",\")\n",
    "            resume_skills = unique_skills(get_skills(input_resume.lower()))\n",
    "            score = 0\n",
    "            for x in req_skills:\n",
    "                if x in resume_skills:\n",
    "                    score += 1\n",
    "            req_skills_len = len(req_skills)\n",
    "            match = round(score / req_skills_len * 100, 1)\n",
    "            result_textbox.delete(1.0, tk.END)\n",
    "            result_textbox.insert(tk.END, f\"The current Resume is {match}% matched to your requirements\\n{resume_skills}\")\n",
    "\n",
    "# Create a label for the user to enter their desired skills\n",
    "tag_label = ttk.Label(root, text=\"Enter desired skills (comma-separated): \")\n",
    "tag_label.pack()\n",
    "\n",
    "# Create an entry widget for the user to enter their desired skills\n",
    "tag_entry = ttk.Entry(root)\n",
    "tag_entry.pack()\n",
    "\n",
    "# Create a label for the file chooser button\n",
    "file_label = ttk.Label(root, text=\"Upload Resume:\")\n",
    "file_label.pack()\n",
    "\n",
    "# Create a file chooser button\n",
    "file_button = ttk.Button(root, text=\"Choose File\", command=choose_file)\n",
    "file_button.pack()\n",
    "\n",
    "# Create a label to display the result\n",
    "result_label = ttk.Label(root, text=\"Result: \")\n",
    "result_label.pack()\n",
    "\n",
    "# Create a text box to display the result\n",
    "result_textbox = tk.Text(root, height=10, width=50)\n",
    "result_textbox.pack()\n",
    "\n",
    "# Create an exit button to close the Tkinter window\n",
    "exit_button = ttk.Button(root, text=\"Exit\", command=root.destroy)\n",
    "exit_button.pack()\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resume Recommender**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_resume_skills(resume_text):\n",
    "    resume_skills = get_skills(resume_text.lower())\n",
    "    return unique_skills(resume_skills)\n",
    "\n",
    "\n",
    "def calculate_match_score(req_skills, resume_skills):\n",
    "    score = 0\n",
    "    for skill in req_skills:\n",
    "        if skill in resume_skills:\n",
    "            score += 1\n",
    "    req_skills_len = len(req_skills)\n",
    "    match_score = round(score / req_skills_len * 100, 1)\n",
    "    return match_score\n",
    "\n",
    "\n",
    "def extract_resume_text(pdf_path):\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        resume_text = ''\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            resume_text += page.extract_text()\n",
    "        resume_skills = get_resume_skills(resume_text)\n",
    "        return resume_text, resume_skills\n",
    "\n",
    "\n",
    "def process_resumes(resumes_folder, req_skills, ground_truth_file=None):\n",
    "    resumes = os.listdir(resumes_folder)\n",
    "    match_scores = []\n",
    "    for resume_file in resumes:\n",
    "        if not resume_file.endswith('.pdf'):\n",
    "            continue\n",
    "        resume_path = os.path.join(resumes_folder, resume_file)\n",
    "        resume_text, resume_skills = extract_resume_text(resume_path)\n",
    "        match_score = calculate_match_score(req_skills, resume_skills)\n",
    "        match_scores.append((match_score, resume_file, resume_skills))\n",
    "\n",
    "    # Sort resumes by match score\n",
    "    top_resumes = sorted(match_scores, reverse=True)[:10] # Get top 10 resumes with highest match scores\n",
    "\n",
    "    # If ground truth file is provided, calculate precision, recall, and F1 score\n",
    "    if ground_truth_file is not None:\n",
    "        ground_truth = pd.read_csv(ground_truth_file)\n",
    "        relevant_resumes = set(ground_truth[ground_truth['relevant'] == 1]['resume'].values)\n",
    "        selected_resumes = set([resume_file for _, resume_file, _ in top_resumes])\n",
    "        true_positives = len(relevant_resumes & selected_resumes)\n",
    "        false_positives = len(selected_resumes - relevant_resumes)\n",
    "        false_negatives = len(relevant_resumes - selected_resumes)\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        \n",
    "        if precision != 0 or recall != 0:\n",
    "            f1_score = 2 * precision * recall / (precision + recall)\n",
    "        else:\n",
    "            f1_score = 0\n",
    "\n",
    "        return top_resumes, precision, recall, f1_score\n",
    "    else:\n",
    "        return top_resumes, None, None, None\n",
    "\n",
    "\n",
    "def process_input():\n",
    "    # Get the input skills from the user\n",
    "    input_skills = input_skills_entry.get().lower().split(\",\")\n",
    "\n",
    "    # Get the resumes folder from the user using a file dialog\n",
    "    resumes_folder = filedialog.askdirectory(title=\"Select Resumes Folder\")\n",
    "\n",
    "    # Get the ground truth file from the user using a file dialog\n",
    "    ground_truth_file = filedialog.askopenfilename(title=\"Select Ground Truth File\",\n",
    "                                                   filetypes=[(\"CSV Files\", \"*.csv\")])\n",
    "\n",
    "    # Process the resumes\n",
    "    top_resumes, precision, recall, f1_score = process_resumes(resumes_folder, input_skills, ground_truth_file)\n",
    "\n",
    "    # Print the results to the text widget\n",
    "    result_text.delete(\"1.0\", tk.END)\n",
    "    result_text.insert(tk.END, \"Top Resumes:\\n\")\n",
    "    for i, (match_score, resume_file, resume_skills) in enumerate(top_resumes):\n",
    "        result_text.insert(tk.END, f\"{i+1}. {resume_file} (Match Score: {match_score}%)\\n\")\n",
    "        result_text.insert(tk.END, f\"Skills: {', '.join(resume_skills)}\\n\\n\")\n",
    "\n",
    "    if precision is not None and recall is not None and f1_score is not None:\n",
    "        result_text.insert(tk.END, f\"Precision: {precision}\\n\")\n",
    "        result_text.insert(tk.END, f\"Recall: {recall}\\n\")\n",
    "        result_text.insert(tk.END, f\"F1 Score: {f1_score}\\n\")\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Resume Matcher\")\n",
    "\n",
    "# Create the input skills label and entry widget\n",
    "input_skills_label = tk.Label(root, text=\"Enter required skills (comma-separated): \")\n",
    "input_skills_label.grid(row=0, column=0, padx=10, pady=10)\n",
    "input_skills_entry = tk.Entry(root)\n",
    "input_skills_entry.grid(row=0, column=1, padx=10, pady=10)\n",
    "\n",
    "# Create the resumes folder label and button\n",
    "resumes_folder_label = tk.Label(root, text=\"Select Resumes Folder: \")\n",
    "resumes_folder_label.grid(row=1, column=0, padx=10, pady=10)\n",
    "resumes_folder_button = tk.Button(root, text=\"Browse\", command=lambda: input_folder.set(filedialog.askdirectory(title=\"Select Resumes Folder\")))\n",
    "resumes_folder_button.grid(row=1, column=1, padx=10, pady=10)\n",
    "input_folder = tk.StringVar()\n",
    "input_folder.set(\"\")\n",
    "\n",
    "# Create the ground truth file label and button\n",
    "ground_truth_label = tk.Label(root, text=\"Select Ground Truth File: \")\n",
    "ground_truth_label.grid(row=2, column=0, padx=10, pady=10)\n",
    "ground_truth_button = tk.Button(root, text=\"Browse\", command=lambda: input_file.set(filedialog.askopenfilename(title=\"Select Ground Truth File\", filetypes=[(\"CSV Files\", \"*.csv\")])))\n",
    "ground_truth_button.grid(row=2, column=1, padx=10, pady=10)\n",
    "input_file = tk.StringVar()\n",
    "input_file.set(\"\")\n",
    "\n",
    "# Create the submit button\n",
    "submit_button = tk.Button(root, text=\"Submit\", command=process_input)\n",
    "submit_button.grid(row=3, column=0, padx=10, pady=10)\n",
    "\n",
    "# Create the result text widget\n",
    "result_text = tk.Text(root, width=80, height=20)\n",
    "result_text.grid(row=4, column=0, columnspan=2, padx=10, pady=10)\n",
    "\n",
    "# Start the main loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Topic Modeling - LDA**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data[\"Clean_Resume\"].values\n",
    "dictionary = corpora.Dictionary(d.split() for d in docs)\n",
    "bow = [dictionary.doc2bow(d.split()) for d in docs]\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "num_topics = 4\n",
    "ldamodel = lda(\n",
    "    bow, \n",
    "    num_topics=num_topics, \n",
    "    id2word=dictionary, \n",
    "    passes=50, \n",
    "    minimum_probability=0\n",
    ")\n",
    "ldamodel.print_topics(num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyLDAvis\n",
    "The best way to visualize Topics is to use pyLDAvis from GENSIM.\n",
    "\n",
    "* topic #1 appears to relate to the project,management,system.\n",
    "* topic #2 relates to management,company,business.\n",
    "* topic #3 relates to customer,services , state.\n",
    "* topic #4 relates to state,city,student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyLDAvis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpyLDAvis\u001b[49m.enable_notebook()\n\u001b[32m      2\u001b[39m pyLDAvis.gensim_models.prepare(ldamodel, bow, dictionary)\n",
      "\u001b[31mNameError\u001b[39m: name 'pyLDAvis' is not defined"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim_models.prepare(ldamodel, bow, dictionary)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Resume_Analysis_With_Spacy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
