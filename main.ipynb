{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LhOslm_cswec"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Dataset : https://www.kaggle.com/snehaanbhawal/resume-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rX7aN7T2tXSI"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2DFo5vagMO4",
    "outputId": "0dc81236-df8e-406b-fb24-c65ab155d7e0"
   },
   "outputs": [],
   "source": [
    "# ! pip install spacy gensim pyLDAvis wordcloud plotly nltk\n",
    "import nltk\n",
    "nltk.download(['stopwords','wordnet'])\n",
    "#spacy\n",
    "# ! pip install spacy\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "#gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "#Visualization\n",
    "from spacy import displacy\n",
    "import pyLDAvis.gensim_models\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Data loading/ Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# ! pip install jsonlines\n",
    "import jsonlines\n",
    "\n",
    "#nltk\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(['stopwords','wordnet'])\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import IPython.display\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ex-KFJOGuOYz"
   },
   "source": [
    "# Resume Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkZ0WlFEtaac"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Resume.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "v7273AIC5VUf",
    "outputId": "88e64ba3-cb93-4c79-812f-80ada8466e09"
   },
   "outputs": [],
   "source": [
    "df = df.reindex(np.random.permutation(df.index))\n",
    "data = df.copy().iloc[0:1000,]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0XtXArh518b"
   },
   "outputs": [],
   "source": [
    "! python -m spacy download en_core_web_lg\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHU85zLf8ICz"
   },
   "outputs": [],
   "source": [
    "\n",
    "skill_pattern_path = \"jz_skill_patterns.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entity Ruler**\n",
    "To create an entity ruler we need to add a pipeline and then load the .jsonl file containing skills into ruler. As you can see we have successfully added a new pipeline entity_ruler. Entity ruler helps us add additional rules to highlight various categories within the text, such as skills and job description in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "wJOYyfAl8Iga",
    "outputId": "f968af29-2fa3-41f0-f1ff-1757b240b4ec"
   },
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.from_disk(skill_pattern_path)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Skills**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOU9-DSH_A88"
   },
   "outputs": [],
   "source": [
    "def get_skills(text):\n",
    "    doc = nlp(text)\n",
    "    myset = []\n",
    "    subset = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"SKILL\":\n",
    "            subset.append(ent.text)\n",
    "    myset.append(subset)\n",
    "    return subset\n",
    "\n",
    "\n",
    "def unique_skills(x):\n",
    "    return list(set(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Resume \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = []\n",
    "for i in range(data.shape[0]):\n",
    "    review = re.sub(\n",
    "        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n",
    "        \" \",\n",
    "        data[\"Resume_str\"].iloc[i],\n",
    "    )\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    lm = WordNetLemmatizer()\n",
    "    review = [\n",
    "        lm.lemmatize(word)\n",
    "        for word in review\n",
    "        if not word in set(stopwords.words(\"english\"))\n",
    "    ]\n",
    "    review = \" \".join(review)\n",
    "    clean.append(review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Applying functions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Clean_Resume\"] = clean\n",
    "data[\"skills\"] = data[\"Clean_Resume\"].str.lower().apply(get_skills)\n",
    "data[\"skills\"] = data[\"skills\"].apply(unique_skills)\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Analysis And Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data, x=\"Category\", title=\"Distribution of Jobs Categories\"\n",
    ").update_xaxes(categoryorder=\"total descending\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_cat = data[\"Category\"].unique()\n",
    "Job_cat = np.append(Job_cat, \"ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropdown_menu_widget(Job_Category):\n",
    "    output=widgets.Output()\n",
    "    drop_Job=widgets.Dropdown(options = Job_Category, value=None, description='Job Category:')\n",
    "    \n",
    "    for cat in Job_Category:\n",
    "        def dropdown_Job_eventhandler(change):\n",
    "            display(input_widgets)\n",
    "            job_choice=change.new\n",
    "            IPython.display.clear_output(wait=True)\n",
    "    drop_Job.observe(dropdown_Job_eventhandler,names='value')\n",
    "    input_widgets=widgets.HBox([drop_Job])\n",
    "    display(input_widgets)\n",
    "    IPython.display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdown_menu_widget(Job_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Job_Category=\"INFORMATION-TECHNOLOGY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_skills = []\n",
    "if Job_Category != \"ALL\":\n",
    "    fltr = data[data[\"Category\"] == Job_Category][\"skills\"]\n",
    "    for x in fltr:\n",
    "        for i in x:\n",
    "            Total_skills.append(i)\n",
    "else:\n",
    "    fltr = data[\"skills\"]\n",
    "    for x in fltr:\n",
    "        for i in x:\n",
    "            Total_skills.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    x=Total_skills,\n",
    "    labels={\"x\": \"Skills\"},\n",
    "    title=f\"{Job_Category} Distribution of Skills\",\n",
    ").update_xaxes(categoryorder=\"total descending\")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Most common words**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for i in data[data[\"Category\"] == Job_Category][\"Clean_Resume\"].values:\n",
    "    text += i + \" \"\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "x, y = np.ogrid[:300, :300]\n",
    "\n",
    "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = 255 * mask.astype(int)\n",
    "\n",
    "wc = WordCloud(\n",
    "    width=800,\n",
    "    height=800,\n",
    "    background_color=\"white\",\n",
    "    min_font_size=6,\n",
    "    repeat=True,\n",
    "    mask=mask,\n",
    ")\n",
    "wc.generate(text)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.title(f\"Most Used Words in {Job_Category} Resume\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entity Recognition**\n",
    "We can also display various entities within our raw text by using spaCy displacy.render. I am in love with this function as it is an amazing way to look at your entire document and discover SKILL or GEP within your Resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nlp(data[\"Resume_str\"].iloc[0])\n",
    "displacy.render(sent, style=\"ent\", jupyter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dependency Parsing**\n",
    "We can also visualize dependencies by just changing style to dep as shown below. We have also limited words to 10 which includes space too. Limiting the words will make it visualize the small chunk of data and if you want to see the dependency, you can remove the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(sent[0:10], style=\"dep\", jupyter=False, options={\"distance\": 90})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Custom Entity Recognition**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = df.Category.unique()\n",
    "for a in patterns:\n",
    "    ruler.add_patterns([{\"label\": \"Job-Category\", \"pattern\": a}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = {\n",
    "    \"Job-Category\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n",
    "    \"SKILL\": \"linear-gradient(90deg, #9BE15D, #00E3AE)\",\n",
    "    \"ORG\": \"#ffd966\",\n",
    "    \"PERSON\": \"#e06666\",\n",
    "    \"GPE\": \"#9fc5e8\",\n",
    "    \"DATE\": \"#c27ba0\",\n",
    "    \"ORDINAL\": \"#674ea7\",\n",
    "    \"PRODUCT\": \"#f9cb9c\",\n",
    "}\n",
    "options = {\n",
    "    \"ents\": [\n",
    "        \"Job-Category\",\n",
    "        \"SKILL\",\n",
    "        \"ORG\",\n",
    "        \"PERSON\",\n",
    "        \"GPE\",\n",
    "        \"DATE\",\n",
    "        \"ORDINAL\",\n",
    "        \"PRODUCT\",\n",
    "    ],\n",
    "    \"colors\": colors,\n",
    "}\n",
    "sent = nlp(data[\"Resume_str\"].iloc[5])\n",
    "displacy.render(sent, style=\"ent\", jupyter=False, options=options)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Resume Analysis**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Single Resume Analyzer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tkinter import ttk\n",
    "\n",
    "# Create a Tkinter root window\n",
    "root = tk.Tk()\n",
    "root.title(\"Resume Recommendation System\")\n",
    "\n",
    "style = ttk.Style()\n",
    "style.configure('TLabel', foreground='black', font=('Arial', 12))\n",
    "style.configure('TButton', foreground='black', font=('Arial', 12))\n",
    "style.configure('TEntry', foreground='black', font=('Arial', 12))\n",
    "\n",
    "def choose_file():\n",
    "    \n",
    "    input_skills = tag_entry.get()\n",
    "\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "       \n",
    "        with open(file_path, 'rb') as pdf_file:\n",
    "            \n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            \n",
    "            pdf_text = []\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page_obj = pdf_reader.pages[page_num]\n",
    "                pdf_text.append(page_obj.extract_text())\n",
    "           \n",
    "            input_resume = '\\n'.join(pdf_text)\n",
    "\n",
    "            sent2 = nlp(input_resume)\n",
    "            displacy.render(sent2, style=\"ent\", jupyter=False, options=options)\n",
    "\n",
    "\n",
    "            req_skills = input_skills.lower().split(\",\")\n",
    "            resume_skills = unique_skills(get_skills(input_resume.lower()))\n",
    "            score = 0\n",
    "            for x in req_skills:\n",
    "                if x in resume_skills:\n",
    "                    score += 1\n",
    "            req_skills_len = len(req_skills)\n",
    "            match = round(score / req_skills_len * 100, 1)\n",
    "            result_textbox.delete(1.0, tk.END)\n",
    "            result_textbox.insert(tk.END, f\"The current Resume is {match}% matched to your requirements\\n{resume_skills}\")\n",
    "\n",
    "# Create a label for the user to enter their desired skills\n",
    "tag_label = ttk.Label(root, text=\"Enter desired skills (comma-separated): \")\n",
    "tag_label.pack()\n",
    "\n",
    "tag_entry = ttk.Entry(root)\n",
    "tag_entry.pack()\n",
    "\n",
    "file_label = ttk.Label(root, text=\"Upload Resume:\")\n",
    "file_label.pack()\n",
    "\n",
    "file_button = ttk.Button(root, text=\"Choose File\", command=choose_file)\n",
    "file_button.pack()\n",
    "\n",
    "result_label = ttk.Label(root, text=\"Result: \")\n",
    "result_label.pack()\n",
    "\n",
    "result_textbox = tk.Text(root, height=10, width=50)\n",
    "result_textbox.pack()\n",
    "\n",
    "exit_button = ttk.Button(root, text=\"Exit\", command=root.destroy)\n",
    "exit_button.pack()\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resume Recommender**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\tkinter\\__init__.py\", line 1967, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SAYAN\\AppData\\Local\\Temp\\ipykernel_5432\\568001767.py\", line 77, in process_input\n",
      "    top_resumes, precision, recall, f1_score = process_resumes(resumes_folder, input_skills, ground_truth_file)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SAYAN\\AppData\\Local\\Temp\\ipykernel_5432\\568001767.py\", line 35, in process_resumes\n",
      "    resumes = os.listdir(resumes_folder)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [WinError 3] The system cannot find the path specified: ''\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_resume_skills(resume_text):\n",
    "    resume_skills = get_skills(resume_text.lower())\n",
    "    return unique_skills(resume_skills)\n",
    "\n",
    "\n",
    "def calculate_match_score(req_skills, resume_skills):\n",
    "    score = 0\n",
    "    for skill in req_skills:\n",
    "        if skill in resume_skills:\n",
    "            score += 1\n",
    "    req_skills_len = len(req_skills)\n",
    "    match_score = round(score / req_skills_len * 100, 1)\n",
    "    return match_score\n",
    "\n",
    "\n",
    "def extract_resume_text(pdf_path):\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        resume_text = ''\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            resume_text += page.extract_text()\n",
    "        resume_skills = get_resume_skills(resume_text)\n",
    "        return resume_text, resume_skills\n",
    "\n",
    "\n",
    "def process_resumes(resumes_folder, req_skills, ground_truth_file=None):\n",
    "    resumes = os.listdir(resumes_folder)\n",
    "    match_scores = []\n",
    "    for resume_file in resumes:\n",
    "        if not resume_file.endswith('.pdf'):\n",
    "            continue\n",
    "        resume_path = os.path.join(resumes_folder, resume_file)\n",
    "        resume_text, resume_skills = extract_resume_text(resume_path)\n",
    "        match_score = calculate_match_score(req_skills, resume_skills)\n",
    "        match_scores.append((match_score, resume_file, resume_skills))\n",
    "\n",
    "    # Sort resumes by match score\n",
    "    top_resumes = sorted(match_scores, reverse=True)[:10]\n",
    "\n",
    "    if ground_truth_file is not None:\n",
    "        ground_truth = pd.read_csv(ground_truth_file)\n",
    "        relevant_resumes = set(ground_truth[ground_truth['relevant'] == 1]['resume'].values)\n",
    "        selected_resumes = set([resume_file for _, resume_file, _ in top_resumes])\n",
    "        true_positives = len(relevant_resumes & selected_resumes)\n",
    "        false_positives = len(selected_resumes - relevant_resumes)\n",
    "        false_negatives = len(relevant_resumes - selected_resumes)\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        \n",
    "        if precision != 0 or recall != 0:\n",
    "            f1_score = 2 * precision * recall / (precision + recall)\n",
    "        else:\n",
    "            f1_score = 0\n",
    "\n",
    "        return top_resumes, precision, recall, f1_score\n",
    "    else:\n",
    "        return top_resumes, None, None, None\n",
    "\n",
    "\n",
    "def process_input():\n",
    "    # Get the input skills from the user\n",
    "    input_skills = input_skills_entry.get().lower().split(\",\")\n",
    "\n",
    "    resumes_folder = filedialog.askdirectory(title=\"Select Resumes Folder\")\n",
    "\n",
    "    ground_truth_file = filedialog.askopenfilename(title=\"Select Ground Truth File\",\n",
    "                                                   filetypes=[(\"CSV Files\", \"*.csv\")])\n",
    "\n",
    "    top_resumes, precision, recall, f1_score = process_resumes(resumes_folder, input_skills, ground_truth_file)\n",
    "\n",
    "    result_text.delete(\"1.0\", tk.END)\n",
    "    result_text.insert(tk.END, \"Top Resumes:\\n\")\n",
    "    for i, (match_score, resume_file, resume_skills) in enumerate(top_resumes):\n",
    "        result_text.insert(tk.END, f\"{i+1}. {resume_file} (Match Score: {match_score}%)\\n\")\n",
    "        result_text.insert(tk.END, f\"Skills: {', '.join(resume_skills)}\\n\\n\")\n",
    "\n",
    "    if precision is not None and recall is not None and f1_score is not None:\n",
    "        result_text.insert(tk.END, f\"Precision: {precision}\\n\")\n",
    "        result_text.insert(tk.END, f\"Recall: {recall}\\n\")\n",
    "        result_text.insert(tk.END, f\"F1 Score: {f1_score}\\n\")\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Resume Matcher\")\n",
    "\n",
    "input_skills_label = tk.Label(root, text=\"Enter required skills (comma-separated): \")\n",
    "input_skills_label.grid(row=0, column=0, padx=10, pady=10)\n",
    "input_skills_entry = tk.Entry(root)\n",
    "input_skills_entry.grid(row=0, column=1, padx=10, pady=10)\n",
    "\n",
    "resumes_folder_label = tk.Label(root, text=\"Select Resumes Folder: \")\n",
    "resumes_folder_label.grid(row=1, column=0, padx=10, pady=10)\n",
    "resumes_folder_button = tk.Button(root, text=\"Browse\", command=lambda: input_folder.set(filedialog.askdirectory(title=\"Select Resumes Folder\")))\n",
    "resumes_folder_button.grid(row=1, column=1, padx=10, pady=10)\n",
    "input_folder = tk.StringVar()\n",
    "input_folder.set(\"\")\n",
    "\n",
    "ground_truth_label = tk.Label(root, text=\"Select Ground Truth File: \")\n",
    "ground_truth_label.grid(row=2, column=0, padx=10, pady=10)\n",
    "ground_truth_button = tk.Button(root, text=\"Browse\", command=lambda: input_file.set(filedialog.askopenfilename(title=\"Select Ground Truth File\", filetypes=[(\"CSV Files\", \"*.csv\")])))\n",
    "ground_truth_button.grid(row=2, column=1, padx=10, pady=10)\n",
    "input_file = tk.StringVar()\n",
    "input_file.set(\"\")\n",
    "\n",
    "\n",
    "submit_button = tk.Button(root, text=\"Submit\", command=process_input)\n",
    "submit_button.grid(row=3, column=0, padx=10, pady=10)\n",
    "\n",
    "result_text = tk.Text(root, width=80, height=20)\n",
    "result_text.grid(row=4, column=0, columnspan=2, padx=10, pady=10)\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Resume_Analysis_With_Spacy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
